{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Advertising & Pricing - Multiple Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Restart/')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bidding_environment import *\n",
    "from pricing_environment import *\n",
    "from ts_learner import *\n",
    "from gpts_learner import *\n",
    "from data import p, n_for_b, ad_pricing_range_max, p_star\n",
    "from good_knapsack import *\n",
    "import math\n",
    "import pulp\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Sets initial data\n",
    "n_arms_ads = 25 # number of arms for advertising\n",
    "n_arms_pricing = 8 # number of arms for pricing\n",
    "\n",
    "N = 10 # N of experiments\n",
    "T = 200 # T for Times\n",
    "min_bid = 0.0\n",
    "max_bid = 1.0\n",
    "bids = np.linspace(min_bid, max_bid, n_arms_ads) # bids are a linspace\n",
    "\n",
    "sigma = 5\n",
    "\n",
    "# Presets regrets and rewards that are going to be computed\n",
    "regrets_per_subcampaign = []\n",
    "rewards_per_subcampaign = []\n",
    "\n",
    "price_min = 50.0\n",
    "price_max = 70.0\n",
    "\n",
    "prices = np.linspace(price_min, price_max, n_arms_pricing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 21.00it/s]\n",
      "100%|██████████| 200/200 [00:11<00:00, 17.94it/s]\n",
      "100%|██████████| 200/200 [00:11<00:00, 18.10it/s]\n",
      "100%|██████████| 200/200 [00:12<00:00, 15.68it/s]\n",
      "100%|██████████| 200/200 [00:10<00:00, 18.70it/s]\n",
      "100%|██████████| 200/200 [00:12<00:00, 16.61it/s]\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.81it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.95it/s]\n",
      " 72%|███████▏  | 144/200 [00:06<00:02, 20.28it/s]"
     ]
    }
   ],
   "source": [
    "all_regrets = []\n",
    "cum_regrets = []\n",
    "\n",
    "\n",
    "for e in range(N):\n",
    "    \n",
    "    ad_envs = [BiddingEnvironment(bids, sigma, subcampaign=subcampaign) for subcampaign in [1,2,3]]\n",
    "    meta_gpts_learners = [GPTS_Learner(n_arms=n_arms_ads, arms=bids, plain_gp=True) for subcampaign in [1,2,3]]\n",
    "    allocations = [\n",
    "        {1:20, 2:2, 3:2}\n",
    "    ]\n",
    "\n",
    "\n",
    "    pricing_envs = [PricingEnvironment(n_arms=n_arms_pricing, prices=prices, p=p_star, subcampaign=subcampaign) for subcampaign in [1,2,3]]\n",
    "    ts_learners = [TS_Learner(n_arms=n_arms_pricing) for subcampaign in [1,2,3]]\n",
    "\n",
    "    single_ts_learner = TS_Learner(n_arms=n_arms_pricing)\n",
    "\n",
    "    meta_rew = [\n",
    "        ad_envs[0].means * np.max(pricing_envs[0].prices*pricing_envs[0].probabilities(pricing_envs[0].prices)) - bids*ad_pricing_range_max[1] ,\n",
    "        ad_envs[1].means * np.max(pricing_envs[1].prices*pricing_envs[1].probabilities(pricing_envs[1].prices)) - bids*ad_pricing_range_max[2] ,\n",
    "        ad_envs[2].means * np.max(pricing_envs[2].prices*pricing_envs[2].probabilities(pricing_envs[2].prices)) - bids*ad_pricing_range_max[3] \n",
    "    ]\n",
    "\n",
    "    best = good_knapsack(bids, meta_rew, 1.0)\n",
    "    regret_for_ads = [[],[],[]]\n",
    "\n",
    "    restart = 10\n",
    "    cutoff = 80\n",
    "    \n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # 3 subcampaigns:\n",
    "        rewards_per_subcampaign = []\n",
    "        if t < cutoff and t % restart == 0:\n",
    "            r1 = random.choice(range(n_arms_ads))\n",
    "            r2 = random.choice(range(n_arms_ads))\n",
    "            r3 = random.choice(range(n_arms_ads))\n",
    "\n",
    "            allocations.append({\n",
    "                1: r1,\n",
    "                2: r2,\n",
    "                3: r3\n",
    "            })\n",
    "        for subcampaign in [1, 2, 3]:\n",
    "            ad_bid_to_try = allocations[-1][subcampaign] # pull the allocated arm\n",
    "            n_clicks = ad_envs[subcampaign-1].round(ad_bid_to_try) # gets another random value from it\n",
    "            rewards_from_ad = 0\n",
    "\n",
    "            for nc in range(int(n_clicks)):\n",
    "                price_to_try = ts_learners[subcampaign-1].pull_arm()\n",
    "                \n",
    "                reward = pricing_envs[subcampaign-1].round(price_to_try)\n",
    "                rewards_from_ad += (reward*prices[price_to_try])\n",
    "                \n",
    "                ts_learners[subcampaign-1].update(price_to_try, reward)\n",
    "\n",
    "            rewards_from_ad -= (bids[ad_bid_to_try] * ad_pricing_range_max[subcampaign] )\n",
    "            regret_for_ads[subcampaign-1].append(meta_rew[subcampaign-1][best[subcampaign]] - rewards_from_ad)\n",
    "\n",
    "            meta_gpts_learners[subcampaign-1].update(ad_bid_to_try, rewards_from_ad) # updates the learner\n",
    "            # Appends to the rewards the values at lower CI\n",
    "            rewards_per_subcampaign.append(np.random.normal(meta_gpts_learners[subcampaign-1].means, meta_gpts_learners[subcampaign-1].sigmas))\n",
    "        allocations.append(good_knapsack(bids, rewards_per_subcampaign, 1.0))\n",
    "\n",
    "    all_regrets.append(regret_for_ads)\n",
    "    cum_regrets.append(np.cumsum(np.sum(regret_for_ads, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Regrets - 1 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sum(regret_for_ads, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regrets - 1 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(np.sum(regret_for_ads, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regrets - Multiple Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for r in all_regrets:\n",
    "        res.append(np.cumsum(np.sum(r, axis=0)))\n",
    "        \n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.mean(res, axis=0))\n",
    "plt.grid()\n",
    "plt.title('Cumulative Regret (5 experiments) - Multiple Prices', fontsize=20)\n",
    "plt.xlabel('Iteration', fontsize=18)\n",
    "plt.ylabel('Cumulative Regret', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
